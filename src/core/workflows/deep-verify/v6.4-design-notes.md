# V6.4 Design Notes - Using 12 Methods

## Applied Methods
- **core:** #71 First Principles, #152 Socratic Decomposition
- **coherence:** #91 Camouflage Test, #93 DNA Inheritance, #99 Multi-Artifact Coherence
- **meta:** #131 Observer Paradox, #132 Goodhart's Law, #136 Kernel Paradox
- **epistemology:** #111 Godel Witness, #119 Ground Truth Demand
- **protocol:** #141 Method Selection, #150 Learning Extraction

---

## #71 First Principles Analysis

**Question:** What is fundamentally true about verification?

**Fundamentals:**
1. Verification = finding errors (not confirming correctness)
2. Errors have types (10 categories)
3. Methods detect specific error types (not universal)
4. Artifacts have features (type, domain, complexity, dependencies)
5. Effectiveness is measurable (findings/method, confirmed/found)

**Conclusion:** Workflow should map `artifact features â†’ error types â†’ detecting methods`

---

## #152 Socratic Decomposition

### Atomic Questions

| Q# | Question | Answer |
|----|----------|--------|
| Q1 | How does workflow know which methods to use? | Currently: hardcoded. Should: based on artifact features |
| Q2 | How does workflow know when to stop? | Currently: Bayesian check (v6.2+). Works. Keep. |
| Q3 | How does workflow know if methods worked? | Currently: doesn't track. Should: feedback loop |
| Q4 | What makes an artifact need more methods? | Complexity, external deps, security domain, concurrency |
| Q5 | What makes an artifact need fewer methods? | Simple, self-contained, no security aspect |
| Q6 | How to avoid wasting tokens on irrelevant methods? | Match method category to artifact features |

### Consistency Check
- Q1 vs Q6: **CONSISTENT** - both point to feature-based selection
- Q3 vs Q6: **TENSION** - need feedback to know what's "irrelevant"

### Targeted 5 Whys (on Q3 - feedback gap)

```
Problem: Workflow doesn't track method effectiveness

WHY 1: Why doesn't it track?
â†’ No mechanism to record which method found which error

WHY 2: Why no mechanism?
â†’ Workflow designed as linear process, not learning system

WHY 3: Why linear?
â†’ Original v1-v5 assumed fixed best practices

WHY 4: Why fixed best practices?
â†’ Based on static analysis of method categories

WHY 5: Why static?
â†’ ROOT CAUSE: No feedback loop from verification results to method selection
```

**Action:** Add Effectiveness Tracker as core component

---

## #91 Camouflage Test (coherence)

**Question:** Would v6.4 look foreign to someone who knows v6.3?

| Element | v6.3 | v6.4 | Foreign? |
|---------|------|------|----------|
| Phase structure | 0â†’1â†’1.5â†’2â†’3â†’4â†’4.5â†’5â†’6â†’6.5â†’7 | Same | No |
| Self-Check | Phase 0 | Phase 0 | No |
| Integration Check | Phase 1.5 | Phase 1.5 | No |
| Method lists | Hardcoded per layer | **Parameterized** | **YES - but intentional** |
| Bayesian stop | Phase 4.5 | Phase 4.5 | No |
| Kernel handoff | Phase 6.5 | Phase 6.5 | No |

**Verdict:** One intentional change (method selection), rest preserved.

---

## #93 DNA Inheritance Check (coherence)

**System genes to inherit:**

| Gene | v6.3 Pattern | v6.4 Inheritance |
|------|--------------|------------------|
| Phase naming | "Phase N: Name" | INHERIT |
| Checkpoint format | [C] Correct [E] Edit [X] Exit | INHERIT |
| Finding format | ### [N] ðŸ”´ðŸŸ ðŸŸ¡ [DEPTH] Title | INHERIT |
| Severity levels | CRITICAL/IMPORTANT/MINOR | INHERIT |
| Layer structure | A/B/C/D (Content/Structure/Assumptions/Security) | INHERIT |

**Mutations:**
- Phase 3 (Method Selection): MUTATE from fixed list to parameterized criteria

---

## #99 Multi-Artifact Coherence (coherence)

**Related artifacts:**
1. workflow-v6.3.md - parent workflow
2. methods.csv - method catalog
3. adaptive-method-selection.md - selection protocol
4. method_scores.yaml - effectiveness data (new)

**Reference integrity:**
- v6.4 must reference methods by ID from methods.csv âœ“
- Selection rules must use categories from methods.csv âœ“
- Scores must map to method IDs âœ“

---

## #131 Observer Paradox (meta)

**Question:** Is this design GENUINE or PERFORMANCE?

**Signs of performance:**
- Too smooth? NO - has explicit tensions (Q3 vs Q6)
- Too complete? NO - acknowledges unknowns
- Too confident? NO - uses "should" not "will"

**Signs of genuine:**
- Admitted uncertainty? YES - feedback loop effectiveness unknown
- Visible struggle? YES - tension between simplicity and adaptiveness
- Revision marks? YES - this is iterative design

**Verdict:** Genuine design work

---

## #132 Goodhart's Law Check (meta)

**Question:** Am I optimizing metric or actual goal?

| Metric being optimized | Actual goal |
|------------------------|-------------|
| Detection Rate (DR) | Find real errors |
| Token Efficiency (TE) | Don't waste compute |
| Method diversity | Cover all error types |

**Divergence check:**
- Could score high DR while missing real errors? YES - if ground-truth incomplete
- Could score high TE while skipping needed methods? YES - if complexity underestimated

**Mitigation:**
- Use confirmed findings, not just found
- Complexity estimation must be conservative (err toward more methods)

---

## #136 Kernel Paradox (meta)

**Question:** What can't this design verify about itself?

| Item | Why unverifiable | User action required |
|------|------------------|---------------------|
| Context Analyzer accuracy | Self-reference | User reviews feature extraction |
| Method Selector quality | No ground truth | A/B test against v6.3 |
| Score convergence | Long-term behavior | Monitor over 50+ sessions |
| Complexity estimation | Subjective | User overrides available |

---

## #111 Godel Witness (epistemology)

**Question:** What are fundamental limits of adaptive verification?

**Limits:**
1. **No oracle for "right" methods** - can only learn from outcomes
2. **Cold start problem** - no data for new artifact types
3. **Feedback delay** - confirmation happens after verification
4. **Distribution shift** - past effectiveness may not predict future

**Classification:**
- Limits 1, 3, 4: FUNDAMENTAL (cannot solve, only mitigate)
- Limit 2: FIXABLE (use priors from similar types)

---

## #119 Ground Truth Demand (epistemology)

**Claims in this design:**

| Claim | Verifiable? | How? |
|-------|-------------|------|
| "Feature-based selection works" | Externally | A/B test |
| "Tokens will reduce 40-60%" | Self | Measure actual usage |
| "Effectiveness improves over time" | Externally | Track scores over sessions |
| "3 categories minimum ensures coverage" | Unverifiable | Heuristic, needs validation |

**Action:** Mark "3 categories" as tunable parameter, not fixed rule

---

## #141 Method Selection Protocol

**For v6.4 itself, which approach?**

| Need | Constraint | Selected Approach |
|------|------------|-------------------|
| VERIFY + GENERATE | Must work with existing workflows | Parameterized phase |
| Backward compatible | Users know v6.3 | Keep structure, change method lists |
| Token efficient | Reduce 40-60% | Adaptive depth |

---

## #150 Learning Extraction

**From v6.3 testing, what worked?**

| Method | Effectiveness | Keep in v6.4? |
|--------|---------------|---------------|
| #127 Bootstrap Paradox | High (caught INTEGRATE) | YES - conditional on has_external_deps |
| #113 Counterfactual Self-Incrimination | Medium | YES - always in Phase 0 |
| #132 Goodhart's Law | Low usage | YES - for meta-verification tasks |
| #151 Semantic Entropy | Medium | YES - for high-uncertainty findings |

**What didn't work?**
- Fixed 42-55 methods per run (wasteful)
- No tracking of which methods found which errors

---

## V6.4 Core Changes

Based on 12-method analysis:

### Change 1: Parameterized Method Selection

**Before (v6.3):**
```
| Layer | Mandatory Methods |
| A: Content | #70, #71, #72, #73, #75, #150, #152 |
```

**After (v6.4):**
```
| Layer | Selection Criteria |
| A: Content | categories=[sanity, core], min=3, max=6, conditional=[has_external_deps â†’ #127] |
```

### Change 2: Context Analysis Phase (new Phase 0.5)

```
## Phase 0.5: Context Analysis

ARTIFACT_FEATURES:
- type: [code/document/plan/protocol]
- domain: [extract from content]
- complexity: [estimate: low/medium/high]
- has_external_deps: [true/false]
- has_concurrency: [true/false]
- has_state: [true/false]
- has_security_aspect: [true/false]

METHOD_BUDGET:
- complexity.low â†’ 8-12 methods
- complexity.medium â†’ 12-18 methods
- complexity.high â†’ 18-25 methods
```

### Change 3: Effectiveness Tracking (Phase 7.5)

```
## Phase 7.5: Learning Extraction

For each method used:
- findings_produced: N
- findings_confirmed: N
- severity_sum: N

Update method_scores.yaml with new evidence
```

### Change 4: Minimum Base Methods

Always use (regardless of context):
- #81 Scope Integrity Audit
- #84 Coherence Check
- #113 Counterfactual Self-Incrimination

Conditionally add based on features.
